{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e33cb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hasibul\n",
      "[nltk_data]     Hamim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Hasibul\n",
      "[nltk_data]     Hamim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from googletrans import Translator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from tqdm import tqdm  \n",
    "from googletrans import Translator\n",
    "# Translation\n",
    "from tqdm import tqdm  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from collections import Counter\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3e612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasibul Hamim\\AppData\\Local\\Temp\\ipykernel_13800\\2492820922.py:10: DeprecationWarning: Series._data is deprecated and will be removed in a future version. Use public APIs instead.\n",
      "  df.columns = [header]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "wb = openpyxl.load_workbook('E:/Research Work/Bangla_CyberBulling_2024/Code & dataset/Dataset/Final_Dataset_94k_Bangla_CyberBulling_Combine_Dataset_94k.xlsx')\n",
    "ws = wb['Sheet1']\n",
    "data_rows = []\n",
    "for row in ws['A1':'B94000']:\n",
    "    data_cols = [cell.value for cell in row]\n",
    "    data_rows.append(data_cols)\n",
    "df = pd.DataFrame(data_rows)\n",
    "header = df.iloc[0]\n",
    "df.columns = [header]\n",
    "df = df.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc617a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      " (label,) \n",
      "not bully    33578\n",
      "troll        23193\n",
      "sexual       18026\n",
      "religious    15424\n",
      "threat        3778\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = df['label'].value_counts()\n",
    "print(\"Class Counts:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29152146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments = ' '.join(df['comment'].apply(lambda x: ' '.join(map(str, x))).astype(str))\n",
    "\n",
    "# rgx = r\"[\\u0980-\\u09FF]+\"\n",
    "# wordcloud = WordCloud(font_path='E:/Research_Work/Bangla_CyberBulling_2024/Code & dataset/Dataset/Font/CHANO___.ttf',regexp=rgx, width=2000, height=1300, background_color='white').generate(all_comments)\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63af8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                            comment      label\n",
      "1  হয়তো ❤ আয়মান ভাইয়ের পেইজের এডমিন $% মুনজেরিন আ...      troll\n",
      "2              এতো :) আমাদের জন্য বড় # আনন্দের সংবাদ      troll\n",
      "3  >এজন্যই বলি আ-লীগে ❤ চলে আসুন। সব নির্বাচন-ই স...  not bully\n",
      "4  এটিএম :) কার্ড দেখিয়ে স্টুডেন্ট # হাফপাস নেওয়া...      troll\n",
      "5                       > আমাদের জীবনে কোম সিজি নাই?      troll\n",
      "(label,)\n",
      "not bully    33578\n",
      "troll        23193\n",
      "sexual       18026\n",
      "religious    15424\n",
      "threat        3778\n",
      "Name: count, dtype: int64\n",
      "(93999, 2)\n",
      "comment    0\n",
      "label      0\n",
      "dtype: int64\n",
      "comment    0\n",
      "label      0\n",
      "dtype: int64\n",
      "0                                            comment      label\n",
      "1  হয়তো ❤ আয়মান ভাইয়ের পেইজের এডমিন $% মুনজেরিন আ...      troll\n",
      "2              এতো :) আমাদের জন্য বড় # আনন্দের সংবাদ      troll\n",
      "3  >এজন্যই বলি আ-লীগে ❤ চলে আসুন। সব নির্বাচন-ই স...  not bully\n",
      "4  এটিএম :) কার্ড দেখিয়ে স্টুডেন্ট # হাফপাস নেওয়া...      troll\n",
      "5                       > আমাদের জীবনে কোম সিজি নাই?      troll\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "print(df.head())\n",
    "print(df.iloc[:, -1].value_counts())\n",
    "print(df.shape)\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Handle Missing Values\n",
    "mode = df.iloc[:, -2].value_counts().index[0]\n",
    "df.iloc[:, -2].fillna(mode, inplace=True)\n",
    "print(df.isna().sum())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b833ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                            comment      label nlabel noutput\n",
      "1  হয়তো ❤ আয়মান ভাইয়ের পেইজের এডমিন $% মুনজেরিন আ...      troll      1       1\n",
      "2              এতো :) আমাদের জন্য বড় # আনন্দের সংবাদ      troll      1       1\n",
      "3  >এজন্যই বলি আ-লীগে ❤ চলে আসুন। সব নির্বাচন-ই স...  not bully      0       0\n",
      "4  এটিএম :) কার্ড দেখিয়ে স্টুডেন্ট # হাফপাস নেওয়া...      troll      1       1\n",
      "5                       > আমাদের জীবনে কোম সিজি নাই?      troll      1       1\n",
      "0                                            comment      label nlabel noutput\n",
      "1  হয়তো ❤ আয়মান ভাইয়ের পেইজের এডমিন $% মুনজেরিন আ...      troll      1       1\n",
      "2              এতো :) আমাদের জন্য বড় # আনন্দের সংবাদ      troll      1       1\n",
      "3  >এজন্যই বলি আ-লীগে ❤ চলে আসুন। সব নির্বাচন-ই স...  not bully      0       0\n",
      "4  এটিএম :) কার্ড দেখিয়ে স্টুডেন্ট # হাফপাস নেওয়া...      troll      1       1\n",
      "5                       > আমাদের জীবনে কোম সিজি নাই?      troll      1       1\n"
     ]
    }
   ],
   "source": [
    "# Labeling and Encoding\n",
    "list(df.iloc[:, -1].value_counts().index)\n",
    "df['nlabel'] = df.label.replace(['not bully', 'troll', 'sexual', 'religious', 'threat'], [0, 1, 2, 3, 4])\n",
    "df['noutput'] = df.iloc[:, -1].replace(['bully', 'normal'], [1, 0])\n",
    "print(df.head())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81df50f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/93999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93999/93999 [01:16<00:00, 1228.23it/s]\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "output = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    if df.label.values[i] == 'not bully':\n",
    "        output.append('Normal')\n",
    "    else:\n",
    "        output.append('Bully')\n",
    "\n",
    "df['output'] = output\n",
    "df['noutput'] = df.iloc[:, -1].replace(['Bully', 'Normal'], [1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d14078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>হয়তো ❤ আয়মান ভাইয়ের পেইজের এডমিন $% মুনজেরিন আ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>এতো :) আমাদের জন্য বড় # আনন্দের সংবাদ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;এজন্যই বলি আ-লীগে ❤ চলে আসুন। সব নির্বাচন-ই স...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>এটিএম :) কার্ড দেখিয়ে স্টুডেন্ট # হাফপাস নেওয়া...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&gt; আমাদের জীবনে কোম সিজি নাই?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93995</th>\n",
       "      <td>হিরো আলম হিরো আলম আছে থাকবে ইনশাল্লাহ জনদরদি গ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93996</th>\n",
       "      <td>হিরো আলম,, এগিয়ে যাও</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93997</th>\n",
       "      <td>হিরো আলমকে সাপোর্ট দেওয়ার জন্য অসংখ্য ধন্যবাদ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93998</th>\n",
       "      <td>হিরো ভাই তুমি এগিয়ে য়াও</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93999</th>\n",
       "      <td>হুম ভাও তোমরা এগিয়ে যাও আমরা তোমাদের পিছনে আছি</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0                                                comment\n",
       "1      হয়তো ❤ আয়মান ভাইয়ের পেইজের এডমিন $% মুনজেরিন আ...\n",
       "2                  এতো :) আমাদের জন্য বড় # আনন্দের সংবাদ\n",
       "3      >এজন্যই বলি আ-লীগে ❤ চলে আসুন। সব নির্বাচন-ই স...\n",
       "4      এটিএম :) কার্ড দেখিয়ে স্টুডেন্ট # হাফপাস নেওয়া...\n",
       "5                           > আমাদের জীবনে কোম সিজি নাই?\n",
       "...                                                  ...\n",
       "93995  হিরো আলম হিরো আলম আছে থাকবে ইনশাল্লাহ জনদরদি গ...\n",
       "93996                               হিরো আলম,, এগিয়ে যাও\n",
       "93997  হিরো আলমকে সাপোর্ট দেওয়ার জন্য অসংখ্য ধন্যবাদ...\n",
       "93998                            হিরো ভাই তুমি এগিয়ে য়াও\n",
       "93999     হুম ভাও তোমরা এগিয়ে যাও আমরা তোমাদের পিছনে আছি\n",
       "\n",
       "[93999 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a658a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "055a0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Hasibul\n",
      "[nltk_data]     Hamim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Hasibul\n",
      "[nltk_data]     Hamim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "100%|██████████| 93999/93999 [00:51<00:00, 1814.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93999,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(stopwords.fileids())\n",
    "sw = stopwords.words('bengali')\n",
    "\n",
    "new_stopwords = stopwords.words('english')\n",
    "new_stopwords.append('SampleWord')\n",
    "\n",
    "df.to_csv('df_english1000.csv')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "M = len(df)\n",
    "for k in tqdm(range(M)):\n",
    "    text = df.iloc[:, 0].values[k]\n",
    "    txt = re.sub(r'http\\S+', '', text)\n",
    "    txt = re.sub(r'[!@#$%^&*?><,./\\-+`~|:);(❤}{]', '', txt)\n",
    "\n",
    "    text_tokens = word_tokenize(txt)\n",
    "\n",
    "    def detect_benglish(text_tokens):\n",
    "        benglish = False\n",
    "        for j in range(len(text_tokens)):\n",
    "            for i in range(len(text_tokens[j])):\n",
    "                detector = Translator()\n",
    "                txt = text_tokens[j]\n",
    "                if i < len(txt) - 1:\n",
    "                    if detector.detect(txt[i]).lang == 'bn' and detector.detect(txt[i + 1]).lang == 'en' and txt[\n",
    "                        i].isalpha() == True and txt[i + 1].isalpha() == True:\n",
    "                        benglish = True\n",
    "        return benglish\n",
    "\n",
    "    def handle_benglish(text_tokens):\n",
    "        for j in range(len(text_tokens)):\n",
    "            s = []\n",
    "            for i in range(len(text_tokens[j])):\n",
    "                detector = Translator()\n",
    "                txt = text_tokens[j]\n",
    "                if i < len(txt) - 1:\n",
    "                    if detector.detect(txt[i]).lang == 'bn' and detector.detect(txt[i + 1]).lang == 'en' and txt[\n",
    "                        i].isalpha() == True and txt[i + 1].isalpha() == True:\n",
    "                        s.append(i)\n",
    "                    if txt[i] == '|':\n",
    "                        s.append(i)\n",
    "            for i in range(len(s)):\n",
    "                txt = txt.replace(txt[s[i]], txt[s[i]] + \" \")\n",
    "            text_tokens[j] = txt\n",
    "        tstring = str(' '.join(text_tokens)).lower()\n",
    "        text_tokens = word_tokenize(tstring)\n",
    "        return text_tokens\n",
    "\n",
    "    remove_sw = [word for word in text_tokens if not word in sw]\n",
    "    un_items = np.unique(remove_sw)\n",
    "    r_sw = [wordnet_lemmatizer.lemmatize(w) for w in un_items]\n",
    "    bn_tokens = []\n",
    "\n",
    "    def tanslate_bengali(r_sw):\n",
    "        for i in range(len(r_sw)):\n",
    "            bn_tokens.append(translator.translate(r_sw[i], dest='bn').text)\n",
    "        return bn_tokens\n",
    "\n",
    "    bn_token = r_sw\n",
    "    df.iloc[:, 0].values[k] = ' '.join(bn_token)\n",
    "\n",
    "df.iloc[:, -1].value_counts()\n",
    "df.iloc[:, 0]\n",
    "\n",
    "df[:].iloc[:, 0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a6bffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>nlabel</th>\n",
       "      <th>noutput</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>আইডি আপু। আয়মান এডমিন গিয়েছেন। পেইজের ভাইয়ের ভ...</td>\n",
       "      <td>troll</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আনন্দের এতো বড় সংবাদ</td>\n",
       "      <td>troll</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আলীগে আসুন। এজন্যই নির্বাচন। নির্বাচনই বলি সুষ্ঠু</td>\n",
       "      <td>not bully</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>এটিএম কার্ড দেখিয়ে স্টুডেন্ট হাফপাস</td>\n",
       "      <td>troll</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>কোম জীবনে সিজি</td>\n",
       "      <td>troll</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93995</th>\n",
       "      <td>অবশ্যই আছি আমরাও আলম ইনশাআল্লাহ ইনশাল্লাহ করবো...</td>\n",
       "      <td>not bully</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93996</th>\n",
       "      <td>আলম এগিয়ে যাও হিরো</td>\n",
       "      <td>not bully</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93997</th>\n",
       "      <td>অসংখ্য আপনাকে আলমকে ধন্যবাদ সাপোর্ট হিরো</td>\n",
       "      <td>not bully</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93998</th>\n",
       "      <td>এগিয়ে ভাই হিরো য়াও</td>\n",
       "      <td>not bully</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93999</th>\n",
       "      <td>আছি এগিয়ে তোমরা তোমাদের পিছনে ভাও যাও হুম</td>\n",
       "      <td>not bully</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0                                                comment      label nlabel  \\\n",
       "1      আইডি আপু। আয়মান এডমিন গিয়েছেন। পেইজের ভাইয়ের ভ...      troll      1   \n",
       "2                                   আনন্দের এতো বড় সংবাদ      troll      1   \n",
       "3      আলীগে আসুন। এজন্যই নির্বাচন। নির্বাচনই বলি সুষ্ঠু  not bully      0   \n",
       "4                    এটিএম কার্ড দেখিয়ে স্টুডেন্ট হাফপাস      troll      1   \n",
       "5                                         কোম জীবনে সিজি      troll      1   \n",
       "...                                                  ...        ...    ...   \n",
       "93995  অবশ্যই আছি আমরাও আলম ইনশাআল্লাহ ইনশাল্লাহ করবো...  not bully      0   \n",
       "93996                                 আলম এগিয়ে যাও হিরো  not bully      0   \n",
       "93997           অসংখ্য আপনাকে আলমকে ধন্যবাদ সাপোর্ট হিরো  not bully      0   \n",
       "93998                                 এগিয়ে ভাই হিরো য়াও  not bully      0   \n",
       "93999          আছি এগিয়ে তোমরা তোমাদের পিছনে ভাও যাও হুম  not bully      0   \n",
       "\n",
       "0     noutput  output  \n",
       "1           1   Bully  \n",
       "2           1   Bully  \n",
       "3           0  Normal  \n",
       "4           1   Bully  \n",
       "5           1   Bully  \n",
       "...       ...     ...  \n",
       "93995       0  Normal  \n",
       "93996       0  Normal  \n",
       "93997       0  Normal  \n",
       "93998       0  Normal  \n",
       "93999       0  Normal  \n",
       "\n",
       "[93999 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa673686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from scipy.stats import f_oneway\n",
    "# from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import numpy as np\n",
    "\n",
    "# # Text Vectorization (Count)\n",
    "# count_vectorizer = CountVectorizer(max_features=20000, tokenizer=word_tokenize, stop_words=stopwords.words('english'))\n",
    "# iX_count = count_vectorizer.fit_transform(df.iloc[:5000, 0].values)\n",
    "\n",
    "# # TF-IDF Transformation\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# iX_tfidf = tfidf_transformer.fit_transform(iX_count)\n",
    "\n",
    "# # Perform ANOVA test for feature selection\n",
    "# anova_selector = SelectKBest(score_func=f_classif, k=1000)\n",
    "# iX_tfidf_selected = anova_selector.fit_transform(iX_tfidf, df.iloc[:5000, -3].values)\n",
    "\n",
    "# # Convert sparse matrix to dense array\n",
    "# iX_tfidf_selected_dense = iX_tfidf_selected.toarray()\n",
    "\n",
    "# # Perform ANOVA test on the entire dataset\n",
    "# anova_result = f_oneway(*[iX_tfidf_selected_dense[:, i] for i in range(iX_tfidf_selected_dense.shape[1])])\n",
    "# print(\"\\nANOVA Results:\")\n",
    "# print(\"F-value:\", anova_result.statistic)\n",
    "# print(\"p-value:\", anova_result.pvalue)\n",
    "# print(\"Selected Features Shape:\", iX_tfidf_selected_dense.shape)\n",
    "\n",
    "# # Cross-Validation\n",
    "# kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "# for train_index, test_index in kf.split(iX_tfidf_selected_dense):\n",
    "#     X_train, X_test = iX_tfidf_selected_dense[train_index], iX_tfidf_selected_dense[test_index]\n",
    "#     y_train, y_test = df.iloc[:5000, -3].values[train_index], df.iloc[:5000, -3].values[test_index]\n",
    "\n",
    "#     # Logistic Regression (or any other classifier) and evaluation code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8232fac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasibul Hamim\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hasibul Hamim\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANOVA Results:\n",
      "F-value: 20.988634075338258\n",
      "p-value: 0.0\n",
      "Selected Features Shape: (2250, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import f_oneway\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Text Vectorization (Count)\n",
    "count_vectorizer = CountVectorizer(max_features=20000, tokenizer=word_tokenize, stop_words=stopwords.words('english'))\n",
    "iX_count = count_vectorizer.fit_transform(df.iloc[:5000, 0].values)\n",
    "\n",
    "# TF-IDF Transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "iX_tfidf = tfidf_transformer.fit_transform(iX_count)\n",
    "\n",
    "# Perform ANOVA test for feature selection\n",
    "anova_selector = SelectKBest(score_func=f_classif, k=1000)\n",
    "iX_tfidf_selected = anova_selector.fit_transform(iX_tfidf, df.iloc[:5000, -3].values)\n",
    "\n",
    "# Imbalanced Data Handling\n",
    "cc = InstanceHardnessThreshold(random_state=10, estimator=LogisticRegression())\n",
    "X_resampled, y_resampled = cc.fit_resample(iX_tfidf_selected, df.iloc[:5000, -3].values)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "X_resampled_dense = X_resampled.toarray()\n",
    "\n",
    "# Perform ANOVA test on the entire dataset\n",
    "anova_result = f_oneway(*[X_resampled_dense[:, i] for i in range(X_resampled_dense.shape[1])])\n",
    "print(\"\\nANOVA Results:\")\n",
    "print(\"F-value:\", anova_result.statistic)\n",
    "print(\"p-value:\", anova_result.pvalue)\n",
    "print(\"Selected Features Shape:\", X_resampled_dense.shape)\n",
    "\n",
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_index, test_index in kf.split(X_resampled_dense):\n",
    "    X_train, X_test = X_resampled_dense[train_index], X_resampled_dense[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "\n",
    "    # Logistic Regression (or any other classifier) and evaluation code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6969f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasibul Hamim\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hasibul Hamim\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chi2 Results:\n",
      "chi2-value: 825999.7600932752\n",
      "p-value: 1.0\n",
      "Selected Features Shape: (2250, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import chi2_contingency\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Text Vectorization (Count)\n",
    "count_vectorizer = CountVectorizer(max_features=20000, tokenizer=word_tokenize, stop_words=stopwords.words('english'))\n",
    "iX_count = count_vectorizer.fit_transform(df.iloc[:5000, 0].values)\n",
    "\n",
    "# TF-IDF Transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "iX_tfidf = tfidf_transformer.fit_transform(iX_count)\n",
    "\n",
    "# Perform chi2 test for feature selection\n",
    "chi2_selector = SelectKBest(score_func=chi2, k=1000)\n",
    "iX_tfidf_selected = chi2_selector.fit_transform(iX_tfidf, df.iloc[:5000, -3].values)\n",
    "\n",
    "# Imbalanced Data Handling\n",
    "cc = InstanceHardnessThreshold(random_state=10, estimator=LogisticRegression())\n",
    "X_resampled, y_resampled = cc.fit_resample(iX_tfidf_selected, df.iloc[:5000, -3].values)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "X_resampled_dense = X_resampled.toarray()\n",
    "\n",
    "# Add a small value to the contingency table to avoid zero element\n",
    "X_resampled_dense += np.random.rand(*X_resampled_dense.shape) * 0.0001\n",
    "\n",
    "# Perform chi2 test on the entire dataset\n",
    "chi2_result = chi2_contingency(X_resampled_dense)\n",
    "print(\"\\nchi2 Results:\")\n",
    "print(\"chi2-value:\", chi2_result[0])\n",
    "print(\"p-value:\", chi2_result[1])\n",
    "print(\"Selected Features Shape:\", X_resampled_dense.shape)\n",
    "\n",
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_index, test_index in kf.split(X_resampled_dense):\n",
    "    X_train, X_test = X_resampled_dense[train_index], X_resampled_dense[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "\n",
    "    # Logistic Regression (or any other classifier) and evaluation code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
