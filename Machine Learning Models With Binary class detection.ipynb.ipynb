{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from googletrans import Translator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from tqdm import tqdm  \n",
    "from googletrans import Translator\n",
    "# Translation\n",
    "from tqdm import tqdm  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, OneSidedSelection, InstanceHardnessThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "wb = openpyxl.load_workbook('E:/Research_Work/Bangla_CyberBulling_2024/Code & dataset/dataset/Final_Dataset_94k_Bangla_CyberBulling_Combine_Dataset_94k.xlsx')\n",
    "ws = wb['Sheet1']\n",
    "data_rows = []\n",
    "for row in ws['A1':'B94001']:\n",
    "    data_cols = [cell.value for cell in row]\n",
    "    data_rows.append(data_cols)\n",
    "df = pd.DataFrame(data_rows)\n",
    "header = df.iloc[0]\n",
    "df.columns = [header]\n",
    "df = df.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc617a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['label'].value_counts()\n",
    "print(\"Class Counts:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29152146",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = ' '.join(df['comment'].apply(lambda x: ' '.join(map(str, x))).astype(str))\n",
    "\n",
    "rgx = r\"[\\u0980-\\u09FF]+\"\n",
    "wordcloud = WordCloud(font_path='E:/Research_Work/Bangla_CyberBulling_2024/Code & dataset/Dataset/Font/CHANO___.ttf',regexp=rgx, width=2000, height=1300, background_color='white').generate(all_comments)\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f943f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment= df['comment']\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Applying a function to calculate the number of words in each comment\n",
    "word_counts = np.vectorize(lambda x: len(str(x).split()))(comment)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(word_counts, fill=True, color='skyblue')\n",
    "plt.title('Density Plot of Comments Length Before Preprocessing')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust x-axis ticks at intervals of 5\n",
    "plt.xticks(np.arange(0, max(word_counts)+1, 15))\n",
    "\n",
    "# Adjust y-axis ticks to represent percentages\n",
    "plt.yticks(plt.yticks()[0], [f'{int(t*100)}' for t in plt.yticks()[0]])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "print(df.head())\n",
    "print(df.iloc[:, -1].value_counts())\n",
    "print(df.shape)\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Handle Missing Values\n",
    "mode = df.iloc[:, -2].value_counts().index[0]\n",
    "df.iloc[:, -2].fillna(mode, inplace=True)\n",
    "print(df.isna().sum())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling and Encoding\n",
    "list(df.iloc[:, -1].value_counts().index)\n",
    "df['nlabel'] = df.label.replace(['not bully', 'troll', 'sexual', 'religious', 'threat'], [0, 1, 2, 3, 4])\n",
    "df['noutput'] = df.iloc[:, -1].replace(['bully', 'normal'], [1, 0])\n",
    "print(df.head())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "output = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    if df.label.values[i] == 'not bully':\n",
    "        output.append('Normal')\n",
    "    else:\n",
    "        output.append('Bully')\n",
    "\n",
    "df['output'] = output\n",
    "df['noutput'] = df.iloc[:, -1].replace(['Bully', 'Normal'], [1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d14078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.fileids())\n",
    "sw = stopwords.words('bengali')\n",
    "\n",
    "new_stopwords = stopwords.words('english')\n",
    "new_stopwords.append('SampleWord')\n",
    "\n",
    "df.to_csv('df_english1000.csv')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "M = len(df)\n",
    "for k in tqdm(range(M)):\n",
    "    text = df.iloc[:, 0].values[k]\n",
    "    txt = re.sub(r'http\\S+', '', text)\n",
    "    txt = re.sub(r'[!@#$%^&*?><,./\\-+`~|:);(❤}{]', '', txt)\n",
    "\n",
    "    text_tokens = word_tokenize(txt)\n",
    "\n",
    "    def detect_benglish(text_tokens):\n",
    "        benglish = False\n",
    "        for j in range(len(text_tokens)):\n",
    "            for i in range(len(text_tokens[j])):\n",
    "                detector = Translator()\n",
    "                txt = text_tokens[j]\n",
    "                if i < len(txt) - 1:\n",
    "                    if detector.detect(txt[i]).lang == 'bn' and detector.detect(txt[i + 1]).lang == 'en' and txt[\n",
    "                        i].isalpha() == True and txt[i + 1].isalpha() == True:\n",
    "                        benglish = True\n",
    "        return benglish\n",
    "\n",
    "    def handle_benglish(text_tokens):\n",
    "        for j in range(len(text_tokens)):\n",
    "            s = []\n",
    "            for i in range(len(text_tokens[j])):\n",
    "                detector = Translator()\n",
    "                txt = text_tokens[j]\n",
    "                if i < len(txt) - 1:\n",
    "                    if detector.detect(txt[i]).lang == 'bn' and detector.detect(txt[i + 1]).lang == 'en' and txt[\n",
    "                        i].isalpha() == True and txt[i + 1].isalpha() == True:\n",
    "                        s.append(i)\n",
    "                    if txt[i] == '|':\n",
    "                        s.append(i)\n",
    "            for i in range(len(s)):\n",
    "                txt = txt.replace(txt[s[i]], txt[s[i]] + \" \")\n",
    "            text_tokens[j] = txt\n",
    "        tstring = str(' '.join(text_tokens)).lower()\n",
    "        text_tokens = word_tokenize(tstring)\n",
    "        return text_tokens\n",
    "\n",
    "    remove_sw = [word for word in text_tokens if not word in sw]\n",
    "    un_items = np.unique(remove_sw)\n",
    "    r_sw = [wordnet_lemmatizer.lemmatize(w) for w in un_items]\n",
    "    bn_tokens = []\n",
    "\n",
    "    def tanslate_bengali(r_sw):\n",
    "        for i in range(len(r_sw)):\n",
    "            bn_tokens.append(translator.translate(r_sw[i], dest='bn').text)\n",
    "        return bn_tokens\n",
    "\n",
    "    bn_token = r_sw\n",
    "    df.iloc[:, 0].values[k] = ' '.join(bn_token)\n",
    "\n",
    "df.iloc[:, -1].value_counts()\n",
    "df.iloc[:, 0]\n",
    "\n",
    "df[:].iloc[:, 0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = df['comment']\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Applying a function to calculate the number of words in each comment\n",
    "word_counts = np.vectorize(lambda x: len(str(x).split()))(comment)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(word_counts, shade=True, color='coral')  # Use 'coral' directly\n",
    "plt.title('Density Plot of Comments Length After Preprocessing')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust x-axis ticks at intervals of 5\n",
    "plt.xticks(np.arange(0, max(word_counts)+1, 10))\n",
    "\n",
    "# Adjust y-axis ticks to represent percentages\n",
    "plt.yticks(plt.yticks()[0], [f'{int(t*100)}' for t in plt.yticks()[0]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05389d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorization (Count)\n",
    "count_vectorizer = CountVectorizer(max_features=20000, tokenizer=word_tokenize, stop_words=stopwords.words('english'))\n",
    "iX_count = count_vectorizer.fit_transform(df[:].iloc[:, 0].values).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfTransformer()\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(iX_count)\n",
    "iX_tfidf = tf_transformer.transform(iX_count)\n",
    "\n",
    "iX_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d40a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "by = df.iloc[:, -2].values\n",
    "by.shape\n",
    "\n",
    "my = df.iloc[:, -1].values\n",
    "\n",
    "np.unique(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b61a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced Data Handling\n",
    "cc = InstanceHardnessThreshold(random_state=10, estimator=LogisticRegression())\n",
    "X, y = cc.fit_resample(iX_tfidf, by)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "print('complete')\n",
    "print('train:', X_train.shape, y_train.shape, 'test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD+LR+MLP (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a61377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import is_classifier\n",
    "\n",
    "# Assuming you have X_train, X_test, y_train, y_test defined somewhere in your code\n",
    "\n",
    "# Part 51: Model Building (Stacking Classifier: SGD, MLP, RandomForest)\n",
    "def build_stacking_classifier(X_train, y_train):\n",
    "    global stacking_model\n",
    "    print('Building Stacking Classifier model...')\n",
    "    \n",
    "    # Creating instances of the base classifiers\n",
    "    sgd_classifier = SGDClassifier()\n",
    "    mlp_classifier = MLPClassifier()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    # Check if classifiers support probability estimates\n",
    "    classifiers_with_proba = [(name, clf) for name, clf in [('sgd', sgd_classifier), ('mlp', mlp_classifier), ('rf', rf_classifier)] if is_classifier(clf) and hasattr(clf, \"predict_proba\")]\n",
    "\n",
    "    if not classifiers_with_proba:\n",
    "        raise ValueError(\"None of the base classifiers support probability estimates. Stacking requires probability estimates.\")\n",
    "\n",
    "    # Creating the Stacking Classifier\n",
    "    stacking_model = StackingClassifier(classifiers_with_proba, final_estimator=RandomForestClassifier(), stack_method='auto', n_jobs=-1)\n",
    "    \n",
    "    # Fitting the model\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    \n",
    "    print('Stacking Classifier model built successfully.')\n",
    "    return stacking_model\n",
    "\n",
    "# Building the Stacking Classifier (assuming X_train, X_test, y_train, y_test are defined)\n",
    "stacking_model = build_stacking_classifier(X_train, y_train)\n",
    "\n",
    "# Part 52: Model Evaluation (Stacking Classifier)\n",
    "def evaluate_stacking_classifier(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {acc*100:.2f}%')\n",
    "    \n",
    "    # Additional metrics\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f'Precision: {precision*100:.2f}%')\n",
    "    print(f'Recall: {recall*100:.2f}%')\n",
    "    print(f'F1 Score: {f1*100:.2f}%')\n",
    "    \n",
    "    confusion_matrix_analysis(model, X_test, y_test)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# Part 53: Confusion Matrix Analysis (Stacking Classifier)\n",
    "def confusion_matrix_analysis(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    class_names = ['Normal', 'Bully']  # Update class names\n",
    "    \n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    # Display values on the plot without box and color\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            plt.text(j + 0.5, i + 0.5, f'{cm[i, j]}', ha='center', va='center', color='black', fontsize=500)\n",
    "\n",
    "    plt.title('Confusion Matrix (Stacking Classifier)')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluating the Stacking Classifier\n",
    "accuracy_stacking = evaluate_stacking_classifier(stacking_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f47f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part 53: Confusion Matrix Analysis (Stacking Classifier)\n",
    "def confusion_matrix_analysis(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    class_names = np.unique(y_train) # Update class names\n",
    "    \n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    # Display values on the plot without box and color\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            plt.text(j + 0.5, i + 0.5, f'{cm[i, j]}', ha='center', va='center', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "    plt.title('Confusion Matrix (Stacking Classifier)')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluating the Stacking Classifier\n",
    "accuracy_stacking = evaluate_stacking_classifier(stacking_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# Part 54: ROC Curve (LightGBM with Default Parameters)\n",
    "def plot_roc_curve(model, X_test, y_test, class_names):\n",
    "    y_probs = model.predict_proba(X_test)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    for i in range(len(class_names)):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, i], pos_label=class_names[i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "class_names = np.unique(y_train)\n",
    "plot_roc_curve(stacking_model, X_test, y_test, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95568b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Part 54: Cross-Validation (Stacking Classifier)\n",
    "def cross_validate_stacking_classifier(model, X, y, cv=5):\n",
    "    print(f'Cross-validating Stacking Classifier with {cv} folds...')\n",
    "    \n",
    "    # Using cross_val_score for cross-validation\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Displaying cross-validation results\n",
    "    print(f'Cross-Validation Accuracy: {scores.mean()*100:.2f}% (±{scores.std()*100:.2f}%)')\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Cross-validating the Stacking Classifier\n",
    "cv_scores = cross_validate_stacking_classifier(stacking_model, X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
